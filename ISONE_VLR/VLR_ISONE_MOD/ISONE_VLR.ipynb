{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RT & DA LMPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database is empty. Using default start date: 2020-12-01\n",
      " Fetching data from 2020-12-01 to 2024-06-30\n",
      "Fetching data for NEI00000100004001 (Hierarchy ID: 1)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000100004001. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"1.41\",\"12/1/2020 12:00:00 AM\"\n",
      "\"10.76\",\"12/1/2020 1:00:00 AM\"\n",
      "\"12.38\",\"12/1/2020 2:00:00 AM\"\n",
      "\"10.84\",\"12/1/2020 3:00:00 AM\"\n",
      "\"13.2\",\"12/1/2020 4:00:00 AM\"\n",
      "\"13.26\",\"12/1/2020 5:00:00 AM\"\n",
      "\"17.96\",\"12/1/2020 6:00:00 AM\"\n",
      "\"11.48\",\"12/1/2020 7:00:00 AM\"\n",
      "\"11.41\",\"12/1/2020 8:00:00 AM\"\n",
      "\"13.45\",\"12/1/2020 9:00:00 AM\"\n",
      "\"18.59\",\"12/1/2020 10:00:00 AM\"\n",
      "\"18.83\",\"12/1/2020 11:00:00 AM\"\n",
      "\"18.5\",\"12/1/2020 12:00:00 PM\"\n",
      "\"16.96\",\"12/1/2020 1:00:00 PM\"\n",
      "\"18.28\",\"12/1/2020 2:00\n",
      "Fetching data for NEI00000100004002 (Hierarchy ID: 2)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000100004002. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"10.28\",\"12/1/2020 12:00:00 AM\"\n",
      "\"13.33\",\"12/1/2020 1:00:00 AM\"\n",
      "\"13.13\",\"12/1/2020 2:00:00 AM\"\n",
      "\"13.12\",\"12/1/2020 3:00:00 AM\"\n",
      "\"14.26\",\"12/1/2020 4:00:00 AM\"\n",
      "\"14.41\",\"12/1/2020 5:00:00 AM\"\n",
      "\"19.4\",\"12/1/2020 6:00:00 AM\"\n",
      "\"13.86\",\"12/1/2020 7:00:00 AM\"\n",
      "\"13.62\",\"12/1/2020 8:00:00 AM\"\n",
      "\"14.16\",\"12/1/2020 9:00:00 AM\"\n",
      "\"19.54\",\"12/1/2020 10:00:00 AM\"\n",
      "\"19.76\",\"12/1/2020 11:00:00 AM\"\n",
      "\"19.33\",\"12/1/2020 12:00:00 PM\"\n",
      "\"18.46\",\"12/1/2020 1:00:00 PM\"\n",
      "\"18.81\",\"12/1/2020 2:\n",
      "Fetching data for NEI00000100004003 (Hierarchy ID: 3)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000100004003. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"10.1\",\"12/1/2020 12:00:00 AM\"\n",
      "\"12.93\",\"12/1/2020 1:00:00 AM\"\n",
      "\"12.75\",\"12/1/2020 2:00:00 AM\"\n",
      "\"12.74\",\"12/1/2020 3:00:00 AM\"\n",
      "\"14.17\",\"12/1/2020 4:00:00 AM\"\n",
      "\"13.95\",\"12/1/2020 5:00:00 AM\"\n",
      "\"18.91\",\"12/1/2020 6:00:00 AM\"\n",
      "\"13.46\",\"12/1/2020 7:00:00 AM\"\n",
      "\"12.94\",\"12/1/2020 8:00:00 AM\"\n",
      "\"13.45\",\"12/1/2020 9:00:00 AM\"\n",
      "\"18.67\",\"12/1/2020 10:00:00 AM\"\n",
      "\"18.75\",\"12/1/2020 11:00:00 AM\"\n",
      "\"18.68\",\"12/1/2020 12:00:00 PM\"\n",
      "\"17.85\",\"12/1/2020 1:00:00 PM\"\n",
      "\"18.37\",\"12/1/2020 2:\n",
      "Fetching data for NEI00000100004004 (Hierarchy ID: 4)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000100004004. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"10.44\",\"12/1/2020 12:00:00 AM\"\n",
      "\"13.45\",\"12/1/2020 1:00:00 AM\"\n",
      "\"13.27\",\"12/1/2020 2:00:00 AM\"\n",
      "\"13.28\",\"12/1/2020 3:00:00 AM\"\n",
      "\"14.43\",\"12/1/2020 4:00:00 AM\"\n",
      "\"14.53\",\"12/1/2020 5:00:00 AM\"\n",
      "\"19.39\",\"12/1/2020 6:00:00 AM\"\n",
      "\"13.83\",\"12/1/2020 7:00:00 AM\"\n",
      "\"13.56\",\"12/1/2020 8:00:00 AM\"\n",
      "\"14.04\",\"12/1/2020 9:00:00 AM\"\n",
      "\"19.25\",\"12/1/2020 10:00:00 AM\"\n",
      "\"19.38\",\"12/1/2020 11:00:00 AM\"\n",
      "\"18.93\",\"12/1/2020 12:00:00 PM\"\n",
      "\"18.18\",\"12/1/2020 1:00:00 PM\"\n",
      "\"18.52\",\"12/1/2020 2\n",
      "Fetching data for NEI00000100004005 (Hierarchy ID: 5)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000100004005. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"10.49\",\"12/1/2020 12:00:00 AM\"\n",
      "\"13.58\",\"12/1/2020 1:00:00 AM\"\n",
      "\"13.4\",\"12/1/2020 2:00:00 AM\"\n",
      "\"13.41\",\"12/1/2020 3:00:00 AM\"\n",
      "\"14.57\",\"12/1/2020 4:00:00 AM\"\n",
      "\"14.68\",\"12/1/2020 5:00:00 AM\"\n",
      "\"19.58\",\"12/1/2020 6:00:00 AM\"\n",
      "\"14\",\"12/1/2020 7:00:00 AM\"\n",
      "\"13.65\",\"12/1/2020 8:00:00 AM\"\n",
      "\"14.09\",\"12/1/2020 9:00:00 AM\"\n",
      "\"19.4\",\"12/1/2020 10:00:00 AM\"\n",
      "\"19.6\",\"12/1/2020 11:00:00 AM\"\n",
      "\"19.13\",\"12/1/2020 12:00:00 PM\"\n",
      "\"18.29\",\"12/1/2020 1:00:00 PM\"\n",
      "\"18.65\",\"12/1/2020 2:00:00\n",
      "Fetching data for NEI00000100004006 (Hierarchy ID: 6)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000100004006. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"10.55\",\"12/1/2020 12:00:00 AM\"\n",
      "\"13.65\",\"12/1/2020 1:00:00 AM\"\n",
      "\"13.46\",\"12/1/2020 2:00:00 AM\"\n",
      "\"13.46\",\"12/1/2020 3:00:00 AM\"\n",
      "\"14.62\",\"12/1/2020 4:00:00 AM\"\n",
      "\"14.75\",\"12/1/2020 5:00:00 AM\"\n",
      "\"19.76\",\"12/1/2020 6:00:00 AM\"\n",
      "\"14.13\",\"12/1/2020 7:00:00 AM\"\n",
      "\"13.84\",\"12/1/2020 8:00:00 AM\"\n",
      "\"14.33\",\"12/1/2020 9:00:00 AM\"\n",
      "\"19.73\",\"12/1/2020 10:00:00 AM\"\n",
      "\"19.93\",\"12/1/2020 11:00:00 AM\"\n",
      "\"19.47\",\"12/1/2020 12:00:00 PM\"\n",
      "\"18.61\",\"12/1/2020 1:00:00 PM\"\n",
      "\"18.97\",\"12/1/2020 2\n",
      "Fetching data for NEI00000100004007 (Hierarchy ID: 7)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000100004007. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"10.47\",\"12/1/2020 12:00:00 AM\"\n",
      "\"13.54\",\"12/1/2020 1:00:00 AM\"\n",
      "\"13.35\",\"12/1/2020 2:00:00 AM\"\n",
      "\"13.36\",\"12/1/2020 3:00:00 AM\"\n",
      "\"14.52\",\"12/1/2020 4:00:00 AM\"\n",
      "\"14.63\",\"12/1/2020 5:00:00 AM\"\n",
      "\"19.61\",\"12/1/2020 6:00:00 AM\"\n",
      "\"14.01\",\"12/1/2020 7:00:00 AM\"\n",
      "\"13.74\",\"12/1/2020 8:00:00 AM\"\n",
      "\"14.23\",\"12/1/2020 9:00:00 AM\"\n",
      "\"19.55\",\"12/1/2020 10:00:00 AM\"\n",
      "\"19.73\",\"12/1/2020 11:00:00 AM\"\n",
      "\"19.27\",\"12/1/2020 12:00:00 PM\"\n",
      "\"18.43\",\"12/1/2020 1:00:00 PM\"\n",
      "\"18.8\",\"12/1/2020 2:\n",
      "Fetching data for NEI00000100004008 (Hierarchy ID: 8)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000100004008. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"10.46\",\"12/1/2020 12:00:00 AM\"\n",
      "\"13.54\",\"12/1/2020 1:00:00 AM\"\n",
      "\"13.34\",\"12/1/2020 2:00:00 AM\"\n",
      "\"13.34\",\"12/1/2020 3:00:00 AM\"\n",
      "\"14.5\",\"12/1/2020 4:00:00 AM\"\n",
      "\"14.64\",\"12/1/2020 5:00:00 AM\"\n",
      "\"19.67\",\"12/1/2020 6:00:00 AM\"\n",
      "\"14.09\",\"12/1/2020 7:00:00 AM\"\n",
      "\"13.83\",\"12/1/2020 8:00:00 AM\"\n",
      "\"14.34\",\"12/1/2020 9:00:00 AM\"\n",
      "\"19.74\",\"12/1/2020 10:00:00 AM\"\n",
      "\"19.94\",\"12/1/2020 11:00:00 AM\"\n",
      "\"19.49\",\"12/1/2020 12:00:00 PM\"\n",
      "\"18.61\",\"12/1/2020 1:00:00 PM\"\n",
      "\"18.95\",\"12/1/2020 2:\n",
      "Fetching data for NEI00006100004001 (Hierarchy ID: 1)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00006100004001. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"12.5\",\"12/1/2020 12:00:00 AM\"\n",
      "\"11.71\",\"12/1/2020 1:00:00 AM\"\n",
      "\"11.29\",\"12/1/2020 2:00:00 AM\"\n",
      "\"11.55\",\"12/1/2020 3:00:00 AM\"\n",
      "\"11.68\",\"12/1/2020 4:00:00 AM\"\n",
      "\"12.96\",\"12/1/2020 5:00:00 AM\"\n",
      "\"20.96\",\"12/1/2020 6:00:00 AM\"\n",
      "\"23.75\",\"12/1/2020 7:00:00 AM\"\n",
      "\"22.32\",\"12/1/2020 8:00:00 AM\"\n",
      "\"21.39\",\"12/1/2020 9:00:00 AM\"\n",
      "\"22\",\"12/1/2020 10:00:00 AM\"\n",
      "\"21.75\",\"12/1/2020 11:00:00 AM\"\n",
      "\"20.94\",\"12/1/2020 12:00:00 PM\"\n",
      "\"20\",\"12/1/2020 1:00:00 PM\"\n",
      "\"20.09\",\"12/1/2020 2:00:00 \n",
      "Fetching data for NEI00006100004002 (Hierarchy ID: 2)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00006100004002. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"13.47\",\"12/1/2020 12:00:00 AM\"\n",
      "\"12.52\",\"12/1/2020 1:00:00 AM\"\n",
      "\"12.06\",\"12/1/2020 2:00:00 AM\"\n",
      "\"12.38\",\"12/1/2020 3:00:00 AM\"\n",
      "\"12.48\",\"12/1/2020 4:00:00 AM\"\n",
      "\"13.66\",\"12/1/2020 5:00:00 AM\"\n",
      "\"21.88\",\"12/1/2020 6:00:00 AM\"\n",
      "\"24.67\",\"12/1/2020 7:00:00 AM\"\n",
      "\"23.12\",\"12/1/2020 8:00:00 AM\"\n",
      "\"22.16\",\"12/1/2020 9:00:00 AM\"\n",
      "\"22.81\",\"12/1/2020 10:00:00 AM\"\n",
      "\"22.57\",\"12/1/2020 11:00:00 AM\"\n",
      "\"21.83\",\"12/1/2020 12:00:00 PM\"\n",
      "\"20.77\",\"12/1/2020 1:00:00 PM\"\n",
      "\"20.8\",\"12/1/2020 2:\n",
      "Fetching data for NEI00006100004003 (Hierarchy ID: 3)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00006100004003. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"13.17\",\"12/1/2020 12:00:00 AM\"\n",
      "\"12.29\",\"12/1/2020 1:00:00 AM\"\n",
      "\"11.92\",\"12/1/2020 2:00:00 AM\"\n",
      "\"12.28\",\"12/1/2020 3:00:00 AM\"\n",
      "\"12.26\",\"12/1/2020 4:00:00 AM\"\n",
      "\"13.28\",\"12/1/2020 5:00:00 AM\"\n",
      "\"20.92\",\"12/1/2020 6:00:00 AM\"\n",
      "\"23.57\",\"12/1/2020 7:00:00 AM\"\n",
      "\"21.97\",\"12/1/2020 8:00:00 AM\"\n",
      "\"21.16\",\"12/1/2020 9:00:00 AM\"\n",
      "\"21.76\",\"12/1/2020 10:00:00 AM\"\n",
      "\"21.5\",\"12/1/2020 11:00:00 AM\"\n",
      "\"20.98\",\"12/1/2020 12:00:00 PM\"\n",
      "\"19.88\",\"12/1/2020 1:00:00 PM\"\n",
      "\"20.2\",\"12/1/2020 2:0\n",
      "Fetching data for NEI00006100004004 (Hierarchy ID: 4)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00006100004004. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"13.59\",\"12/1/2020 12:00:00 AM\"\n",
      "\"12.67\",\"12/1/2020 1:00:00 AM\"\n",
      "\"12.24\",\"12/1/2020 2:00:00 AM\"\n",
      "\"12.57\",\"12/1/2020 3:00:00 AM\"\n",
      "\"12.64\",\"12/1/2020 4:00:00 AM\"\n",
      "\"13.69\",\"12/1/2020 5:00:00 AM\"\n",
      "\"21.57\",\"12/1/2020 6:00:00 AM\"\n",
      "\"24.26\",\"12/1/2020 7:00:00 AM\"\n",
      "\"22.63\",\"12/1/2020 8:00:00 AM\"\n",
      "\"21.66\",\"12/1/2020 9:00:00 AM\"\n",
      "\"22.26\",\"12/1/2020 10:00:00 AM\"\n",
      "\"22.02\",\"12/1/2020 11:00:00 AM\"\n",
      "\"21.29\",\"12/1/2020 12:00:00 PM\"\n",
      "\"20.27\",\"12/1/2020 1:00:00 PM\"\n",
      "\"20.29\",\"12/1/2020 2\n",
      "Fetching data for NEI00006100004005 (Hierarchy ID: 5)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00006100004005. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"13.84\",\"12/1/2020 12:00:00 AM\"\n",
      "\"12.85\",\"12/1/2020 1:00:00 AM\"\n",
      "\"12.4\",\"12/1/2020 2:00:00 AM\"\n",
      "\"12.71\",\"12/1/2020 3:00:00 AM\"\n",
      "\"12.81\",\"12/1/2020 4:00:00 AM\"\n",
      "\"13.99\",\"12/1/2020 5:00:00 AM\"\n",
      "\"21.85\",\"12/1/2020 6:00:00 AM\"\n",
      "\"24.64\",\"12/1/2020 7:00:00 AM\"\n",
      "\"22.9\",\"12/1/2020 8:00:00 AM\"\n",
      "\"21.87\",\"12/1/2020 9:00:00 AM\"\n",
      "\"22.51\",\"12/1/2020 10:00:00 AM\"\n",
      "\"22.28\",\"12/1/2020 11:00:00 AM\"\n",
      "\"21.56\",\"12/1/2020 12:00:00 PM\"\n",
      "\"20.59\",\"12/1/2020 1:00:00 PM\"\n",
      "\"20.61\",\"12/1/2020 2:0\n",
      "Fetching data for NEI00006100004006 (Hierarchy ID: 6)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00006100004006. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"13.89\",\"12/1/2020 12:00:00 AM\"\n",
      "\"12.89\",\"12/1/2020 1:00:00 AM\"\n",
      "\"12.43\",\"12/1/2020 2:00:00 AM\"\n",
      "\"12.74\",\"12/1/2020 3:00:00 AM\"\n",
      "\"12.85\",\"12/1/2020 4:00:00 AM\"\n",
      "\"14.05\",\"12/1/2020 5:00:00 AM\"\n",
      "\"22.21\",\"12/1/2020 6:00:00 AM\"\n",
      "\"24.99\",\"12/1/2020 7:00:00 AM\"\n",
      "\"23.33\",\"12/1/2020 8:00:00 AM\"\n",
      "\"22.31\",\"12/1/2020 9:00:00 AM\"\n",
      "\"22.96\",\"12/1/2020 10:00:00 AM\"\n",
      "\"22.73\",\"12/1/2020 11:00:00 AM\"\n",
      "\"21.99\",\"12/1/2020 12:00:00 PM\"\n",
      "\"20.95\",\"12/1/2020 1:00:00 PM\"\n",
      "\"20.98\",\"12/1/2020 2\n",
      "Fetching data for NEI00006100004007 (Hierarchy ID: 7)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00006100004007. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"13.75\",\"12/1/2020 12:00:00 AM\"\n",
      "\"12.79\",\"12/1/2020 1:00:00 AM\"\n",
      "\"12.34\",\"12/1/2020 2:00:00 AM\"\n",
      "\"12.66\",\"12/1/2020 3:00:00 AM\"\n",
      "\"12.74\",\"12/1/2020 4:00:00 AM\"\n",
      "\"13.87\",\"12/1/2020 5:00:00 AM\"\n",
      "\"22.01\",\"12/1/2020 6:00:00 AM\"\n",
      "\"24.82\",\"12/1/2020 7:00:00 AM\"\n",
      "\"23.2\",\"12/1/2020 8:00:00 AM\"\n",
      "\"22.2\",\"12/1/2020 9:00:00 AM\"\n",
      "\"22.82\",\"12/1/2020 10:00:00 AM\"\n",
      "\"22.58\",\"12/1/2020 11:00:00 AM\"\n",
      "\"21.81\",\"12/1/2020 12:00:00 PM\"\n",
      "\"20.77\",\"12/1/2020 1:00:00 PM\"\n",
      "\"20.79\",\"12/1/2020 2:0\n",
      "Fetching data for NEI00006100004008 (Hierarchy ID: 8)...\n",
      "Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00006100004008. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"13.76\",\"12/1/2020 12:00:00 AM\"\n",
      "\"12.78\",\"12/1/2020 1:00:00 AM\"\n",
      "\"12.31\",\"12/1/2020 2:00:00 AM\"\n",
      "\"12.62\",\"12/1/2020 3:00:00 AM\"\n",
      "\"12.73\",\"12/1/2020 4:00:00 AM\"\n",
      "\"13.94\",\"12/1/2020 5:00:00 AM\"\n",
      "\"22.17\",\"12/1/2020 6:00:00 AM\"\n",
      "\"25.04\",\"12/1/2020 7:00:00 AM\"\n",
      "\"23.43\",\"12/1/2020 8:00:00 AM\"\n",
      "\"22.42\",\"12/1/2020 9:00:00 AM\"\n",
      "\"23.06\",\"12/1/2020 10:00:00 AM\"\n",
      "\"22.83\",\"12/1/2020 11:00:00 AM\"\n",
      "\"22.08\",\"12/1/2020 12:00:00 PM\"\n",
      "\"20.97\",\"12/1/2020 1:00:00 PM\"\n",
      "\"20.99\",\"12/1/2020 2\n",
      "Data inserted successfully into rt_lmps_isone!\n",
      "Data inserted successfully into da_lmps_isone!\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "DB_PARAMS = {\n",
    "    \"dbname\": \"trueprice\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    " \n",
    "headers = {\n",
    "    'Authorization': 'Basic VFJVRUFQSTphM3dTYVVSVA=='  \n",
    "}\n",
    "\n",
    "# Fetching Data for RT and DA LMPS\n",
    "RT_LMPS_MAPPING = {\n",
    "    \"NEI00000100004001\": 1,  # MAINE\n",
    "    \"NEI00000100004002\": 2,  # NEW HAMPSHIRE  \n",
    "    \"NEI00000100004003\": 3,  # VERMONT\n",
    "    \"NEI00000100004004\": 4,  # CONNECTICUT\n",
    "    \"NEI00000100004005\": 5,  # RHODE ISLAND\n",
    "    \"NEI00000100004006\": 6,  # SEMASS\n",
    "    \"NEI00000100004007\": 7,  # WC MASS\n",
    "    \"NEI00000100004008\": 8   # NEMASS BOST\n",
    "}\n",
    "\n",
    "DA_LMPS_MAPPING = {\n",
    "    \"NEI00006100004001\": 1,  # MAINE\n",
    "    \"NEI00006100004002\": 2,  # NEW HAMPSHIRE\n",
    "    \"NEI00006100004003\": 3,  # VERMONT\n",
    "    \"NEI00006100004004\": 4,  # CONNECTICUT\n",
    "    \"NEI00006100004005\": 5,  # RHODE ISLAND\n",
    "    \"NEI00006100004006\": 6,  # SEMASS\n",
    "    \"NEI00006100004007\": 7,  # WC MASS\n",
    "    \"NEI00006100004008\": 8   # NEMASS BOST\n",
    "}\n",
    "\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "\n",
    "def get_last_date(table_name):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_PARAMS)\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"SELECT MAX(date) FROM public.rt_lmps_isone;\"\n",
    "        cursor.execute(query)\n",
    "        last_date = cursor.fetchone()[0] \n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        if last_date is None:\n",
    "            print(f\"Database is empty. Using default start date: 2020-12-01\")\n",
    "            return datetime(2020, 12, 1)\n",
    "        else:\n",
    "            \n",
    "            next_date = last_date + timedelta(days=1)\n",
    "            print(f\"Found last date: {last_date}. Start from {next_date.strftime('%Y-%m-%d')}\")\n",
    "            return next_date  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching last date: {e}. Using default 2020-01-01\")\n",
    "        return datetime(2020, 12, 1)  \n",
    "\n",
    "\n",
    "start_date = get_last_date(\"rt_lmps_isone\")  \n",
    "#start_date = datetime(2020,01,01)\n",
    "#end_date = datetime.today() - timedelta(days=1)  # Ensure end_date is yesterday\n",
    "end_date = datetime(2024,6,30)  # Ensure end_date is yesterday\n",
    "\n",
    "print(f\" Fetching data from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "\n",
    "def fetch_data(symbol_mapping):\n",
    "    all_data = []\n",
    "    \n",
    "    for symbol, hierarchy_id in symbol_mapping.items():\n",
    "        print(f\"Fetching data for {symbol} (Hierarchy ID: {hierarchy_id})...\")\n",
    "\n",
    "    \n",
    "        url = (f\"https://webservice.gvsi.com/api/v3/getintraday?\"\n",
    "               f\"symbols=%23{symbol}&fields=close%2Ctradedatetimeutc&output=csv&includeheaders=true\"\n",
    "               f\"&startdate={start_date.strftime('%m/%d/%Y')}&enddate={end_date.strftime('%m/%d/%Y')}\"\n",
    "               f\"&aggregatetype=0&intradaybarinterval=60&timezone=publisher\")\n",
    "\n",
    "        print(f\"Requesting API from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"API response received for {symbol}. First 500 characters:\\n{response.text[:500]}\")\n",
    "            df = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "            df['datetime'] = pd.to_datetime(df['tradedatetimeutc'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "            df['date'] = df['datetime'].dt.strftime('%Y-%m-%d')  # Convert to YYYY-MM-DD\n",
    "            df['he'] = df['datetime'].dt.hour + 1  # Convert to HE (Hour Ending)\n",
    "            df['data'] = df['close']\n",
    "            df['hierarchy_id'] = hierarchy_id  # Assign hierarchy ID\n",
    "            df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "            df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "            df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "            df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "            df['hour_type'] = df['he'].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "            \n",
    "            def calculate_block_type(row):\n",
    "                if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "                    return \"5x16\"\n",
    "                elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "                    return \"2x16\"\n",
    "                else:\n",
    "                    return \"7x8\"\n",
    "            df['block_type'] = df.apply(calculate_block_type, axis=1)\n",
    "            df = df[['hierarchy_id', 'date', 'year', 'month', 'day', 'day_type', 'he', 'hour_type', 'block_type', 'data']]\n",
    "            all_data.append(df)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {symbol}. HTTP Status: {response.status_code}\")\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "\n",
    "\n",
    "RT_LMPS = fetch_data(RT_LMPS_MAPPING)\n",
    "DA_LMPS = fetch_data(DA_LMPS_MAPPING)\n",
    "\n",
    "#Push Data into PostgreSQL\n",
    "def insert_data_to_db(df, table_name):\n",
    "    if df.empty:\n",
    "        print(f\" No new data to insert for {table_name}.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Drop duplicates on the conflict key to avoid PostgreSQL ON CONFLICT error\n",
    "        df = df.drop_duplicates(subset=['hierarchy_id', 'date', 'he'], keep='last')\n",
    "\n",
    "        # Convert to native Python objects\n",
    "        df = df.astype(object)\n",
    "\n",
    "        conn = psycopg2.connect(**DB_PARAMS)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        columns = ['hierarchy_id', 'date', 'year', 'month', 'day',\n",
    "                   'day_type', 'he', 'hour_type', 'block_type', 'data']\n",
    "\n",
    "        values = df[columns].values.tolist()\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} ({', '.join(columns)})\n",
    "        VALUES %s\n",
    "        ON CONFLICT (hierarchy_id, date, he)\n",
    "        DO UPDATE SET data = EXCLUDED.data\n",
    "        \"\"\"\n",
    "\n",
    "        execute_values(cursor, insert_query, values)\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"Data inserted successfully into {table_name}!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into {table_name}: {e}\")\n",
    "\n",
    "# Insert Data into Database Tables\n",
    "insert_data_to_db(RT_LMPS, \"rt_lmps_isone\")\n",
    "insert_data_to_db(DA_LMPS, \"da_lmps_isone\")\n",
    "\n",
    "print(\"Process completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DA & RT Load Calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database is empty. Using default start date: 2020-12-01\n",
      "Fetching data from 2020-12-01 to 2024-06-30\n",
      "Fetching data for NEI00009700004001 (Hierarchy ID: 1)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00009700004001. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"933\",\"12/1/2020 12:00:00 AM\"\n",
      "\"911.2\",\"12/1/2020 1:00:00 AM\"\n",
      "\"897.6\",\"12/1/2020 2:00:00 AM\"\n",
      "\"899.5\",\"12/1/2020 3:00:00 AM\"\n",
      "\"930\",\"12/1/2020 4:00:00 AM\"\n",
      "\"1007\",\"12/1/2020 5:00:00 AM\"\n",
      "\"1126\",\"12/1/2020 6:00:00 AM\"\n",
      "\"1215.4\",\"12/1/2020 7:00:00 AM\"\n",
      "\"1278.7\",\"12/1/2020 8:00:00 AM\"\n",
      "\"1324.3\",\"12/1/2020 9:00:00 AM\"\n",
      "\"1350.1\",\"12/1/2020 10:00:00 AM\"\n",
      "\"1396.1\",\"12/1/2020 11:00:00 AM\"\n",
      "\"1404.8\",\"12/1/2020 12:00:00 PM\"\n",
      "\"1407.8\",\"12/1/2020 1:00:00 PM\"\n",
      "\"1391.2\",\"12/1/2020\n",
      "Fetching data for NEI00009700004002 (Hierarchy ID: 2)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00009700004002. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"964.3\",\"12/1/2020 12:00:00 AM\"\n",
      "\"928.3\",\"12/1/2020 1:00:00 AM\"\n",
      "\"910\",\"12/1/2020 2:00:00 AM\"\n",
      "\"909.3\",\"12/1/2020 3:00:00 AM\"\n",
      "\"940.3\",\"12/1/2020 4:00:00 AM\"\n",
      "\"1026.5\",\"12/1/2020 5:00:00 AM\"\n",
      "\"1163.6\",\"12/1/2020 6:00:00 AM\"\n",
      "\"1261.5\",\"12/1/2020 7:00:00 AM\"\n",
      "\"1308.4\",\"12/1/2020 8:00:00 AM\"\n",
      "\"1325.4\",\"12/1/2020 9:00:00 AM\"\n",
      "\"1340.8\",\"12/1/2020 10:00:00 AM\"\n",
      "\"1350.2\",\"12/1/2020 11:00:00 AM\"\n",
      "\"1348.7\",\"12/1/2020 12:00:00 PM\"\n",
      "\"1340.8\",\"12/1/2020 1:00:00 PM\"\n",
      "\"1321.6\",\"12/\n",
      "Fetching data for NEI00009700004003 (Hierarchy ID: 3)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00009700004003. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"472.5\",\"12/1/2020 12:00:00 AM\"\n",
      "\"454.8\",\"12/1/2020 1:00:00 AM\"\n",
      "\"446.4\",\"12/1/2020 2:00:00 AM\"\n",
      "\"448.8\",\"12/1/2020 3:00:00 AM\"\n",
      "\"460.6\",\"12/1/2020 4:00:00 AM\"\n",
      "\"497.5\",\"12/1/2020 5:00:00 AM\"\n",
      "\"562.6\",\"12/1/2020 6:00:00 AM\"\n",
      "\"611.3\",\"12/1/2020 7:00:00 AM\"\n",
      "\"598.9\",\"12/1/2020 8:00:00 AM\"\n",
      "\"533\",\"12/1/2020 9:00:00 AM\"\n",
      "\"494\",\"12/1/2020 10:00:00 AM\"\n",
      "\"525.1\",\"12/1/2020 11:00:00 AM\"\n",
      "\"561.7\",\"12/1/2020 12:00:00 PM\"\n",
      "\"572.1\",\"12/1/2020 1:00:00 PM\"\n",
      "\"581.6\",\"12/1/2020 2:00:\n",
      "Fetching data for NEI00009700004004 (Hierarchy ID: 4)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00009700004004. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"2320.3\",\"12/1/2020 12:00:00 AM\"\n",
      "\"2228.5\",\"12/1/2020 1:00:00 AM\"\n",
      "\"2170.4\",\"12/1/2020 2:00:00 AM\"\n",
      "\"2150.9\",\"12/1/2020 3:00:00 AM\"\n",
      "\"2203\",\"12/1/2020 4:00:00 AM\"\n",
      "\"2371\",\"12/1/2020 5:00:00 AM\"\n",
      "\"2677.5\",\"12/1/2020 6:00:00 AM\"\n",
      "\"2938.7\",\"12/1/2020 7:00:00 AM\"\n",
      "\"3078.3\",\"12/1/2020 8:00:00 AM\"\n",
      "\"3107.9\",\"12/1/2020 9:00:00 AM\"\n",
      "\"3042.9\",\"12/1/2020 10:00:00 AM\"\n",
      "\"3026.8\",\"12/1/2020 11:00:00 AM\"\n",
      "\"3049.3\",\"12/1/2020 12:00:00 PM\"\n",
      "\"3043.3\",\"12/1/2020 1:00:00 PM\"\n",
      "\"3083.3\",\"\n",
      "Fetching data for NEI00009700004005 (Hierarchy ID: 5)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00009700004005. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"627.4\",\"12/1/2020 12:00:00 AM\"\n",
      "\"609.5\",\"12/1/2020 1:00:00 AM\"\n",
      "\"596.6\",\"12/1/2020 2:00:00 AM\"\n",
      "\"593.7\",\"12/1/2020 3:00:00 AM\"\n",
      "\"607.6\",\"12/1/2020 4:00:00 AM\"\n",
      "\"642.8\",\"12/1/2020 5:00:00 AM\"\n",
      "\"719.7\",\"12/1/2020 6:00:00 AM\"\n",
      "\"783.4\",\"12/1/2020 7:00:00 AM\"\n",
      "\"825.7\",\"12/1/2020 8:00:00 AM\"\n",
      "\"848.8\",\"12/1/2020 9:00:00 AM\"\n",
      "\"867.3\",\"12/1/2020 10:00:00 AM\"\n",
      "\"881\",\"12/1/2020 11:00:00 AM\"\n",
      "\"865.5\",\"12/1/2020 12:00:00 PM\"\n",
      "\"863.3\",\"12/1/2020 1:00:00 PM\"\n",
      "\"852\",\"12/1/2020 2:00:\n",
      "Fetching data for NEI00009700004006 (Hierarchy ID: 6)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00009700004006. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"1162.9\",\"12/1/2020 12:00:00 AM\"\n",
      "\"1109.4\",\"12/1/2020 1:00:00 AM\"\n",
      "\"1082.5\",\"12/1/2020 2:00:00 AM\"\n",
      "\"1078.2\",\"12/1/2020 3:00:00 AM\"\n",
      "\"1100.4\",\"12/1/2020 4:00:00 AM\"\n",
      "\"1186.5\",\"12/1/2020 5:00:00 AM\"\n",
      "\"1337.8\",\"12/1/2020 6:00:00 AM\"\n",
      "\"1470.9\",\"12/1/2020 7:00:00 AM\"\n",
      "\"1548.7\",\"12/1/2020 8:00:00 AM\"\n",
      "\"1577.8\",\"12/1/2020 9:00:00 AM\"\n",
      "\"1606.6\",\"12/1/2020 10:00:00 AM\"\n",
      "\"1609\",\"12/1/2020 11:00:00 AM\"\n",
      "\"1596.8\",\"12/1/2020 12:00:00 PM\"\n",
      "\"1561.2\",\"12/1/2020 1:00:00 PM\"\n",
      "\"1562.4\"\n",
      "Fetching data for NEI00009700004007 (Hierarchy ID: 7)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00009700004007. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"1407.1\",\"12/1/2020 12:00:00 AM\"\n",
      "\"1353.8\",\"12/1/2020 1:00:00 AM\"\n",
      "\"1322.5\",\"12/1/2020 2:00:00 AM\"\n",
      "\"1314\",\"12/1/2020 3:00:00 AM\"\n",
      "\"1352.8\",\"12/1/2020 4:00:00 AM\"\n",
      "\"1455.7\",\"12/1/2020 5:00:00 AM\"\n",
      "\"1621.3\",\"12/1/2020 6:00:00 AM\"\n",
      "\"1746.8\",\"12/1/2020 7:00:00 AM\"\n",
      "\"1797.7\",\"12/1/2020 8:00:00 AM\"\n",
      "\"1802.3\",\"12/1/2020 9:00:00 AM\"\n",
      "\"1790.2\",\"12/1/2020 10:00:00 AM\"\n",
      "\"1799.9\",\"12/1/2020 11:00:00 AM\"\n",
      "\"1780.4\",\"12/1/2020 12:00:00 PM\"\n",
      "\"1764.9\",\"12/1/2020 1:00:00 PM\"\n",
      "\"1787\",\"\n",
      "Fetching data for NEI00009700004008 (Hierarchy ID: 8)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00009700004008. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"2065.3\",\"12/1/2020 12:00:00 AM\"\n",
      "\"1998.9\",\"12/1/2020 1:00:00 AM\"\n",
      "\"1962.5\",\"12/1/2020 2:00:00 AM\"\n",
      "\"1947.5\",\"12/1/2020 3:00:00 AM\"\n",
      "\"1982.7\",\"12/1/2020 4:00:00 AM\"\n",
      "\"2103.2\",\"12/1/2020 5:00:00 AM\"\n",
      "\"2323.3\",\"12/1/2020 6:00:00 AM\"\n",
      "\"2511.5\",\"12/1/2020 7:00:00 AM\"\n",
      "\"2628.9\",\"12/1/2020 8:00:00 AM\"\n",
      "\"2659.1\",\"12/1/2020 9:00:00 AM\"\n",
      "\"2679.1\",\"12/1/2020 10:00:00 AM\"\n",
      "\"2726\",\"12/1/2020 11:00:00 AM\"\n",
      "\"2730.6\",\"12/1/2020 12:00:00 PM\"\n",
      "\"2657.6\",\"12/1/2020 1:00:00 PM\"\n",
      "\"2608.3\"\n",
      "Fetching data for NEI00000500004001 (Hierarchy ID: 1)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000500004001. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"429.9\",\"12/1/2020 12:00:00 AM\"\n",
      "\"393.3\",\"12/1/2020 1:00:00 AM\"\n",
      "\"384.5\",\"12/1/2020 2:00:00 AM\"\n",
      "\"369.9\",\"12/1/2020 3:00:00 AM\"\n",
      "\"416.5\",\"12/1/2020 4:00:00 AM\"\n",
      "\"491.5\",\"12/1/2020 5:00:00 AM\"\n",
      "\"680.8\",\"12/1/2020 6:00:00 AM\"\n",
      "\"851.8\",\"12/1/2020 7:00:00 AM\"\n",
      "\"986.1\",\"12/1/2020 8:00:00 AM\"\n",
      "\"993.8\",\"12/1/2020 9:00:00 AM\"\n",
      "\"997.1\",\"12/1/2020 10:00:00 AM\"\n",
      "\"1016.8\",\"12/1/2020 11:00:00 AM\"\n",
      "\"1004\",\"12/1/2020 12:00:00 PM\"\n",
      "\"1047.5\",\"12/1/2020 1:00:00 PM\"\n",
      "\"1046.5\",\"12/1/2020\n",
      "Fetching data for NEI00000500004002 (Hierarchy ID: 2)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000500004002. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"801.8\",\"12/1/2020 12:00:00 AM\"\n",
      "\"755.8\",\"12/1/2020 1:00:00 AM\"\n",
      "\"727.2\",\"12/1/2020 2:00:00 AM\"\n",
      "\"750.7\",\"12/1/2020 3:00:00 AM\"\n",
      "\"802.4\",\"12/1/2020 4:00:00 AM\"\n",
      "\"909.1\",\"12/1/2020 5:00:00 AM\"\n",
      "\"1104.6\",\"12/1/2020 6:00:00 AM\"\n",
      "\"1185.1\",\"12/1/2020 7:00:00 AM\"\n",
      "\"1237\",\"12/1/2020 8:00:00 AM\"\n",
      "\"1271.4\",\"12/1/2020 9:00:00 AM\"\n",
      "\"1290.5\",\"12/1/2020 10:00:00 AM\"\n",
      "\"1282.8\",\"12/1/2020 11:00:00 AM\"\n",
      "\"1295\",\"12/1/2020 12:00:00 PM\"\n",
      "\"1318.6\",\"12/1/2020 1:00:00 PM\"\n",
      "\"1291.8\",\"12/1/2\n",
      "Fetching data for NEI00000500004003 (Hierarchy ID: 3)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000500004003. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"374.9\",\"12/1/2020 12:00:00 AM\"\n",
      "\"321.4\",\"12/1/2020 1:00:00 AM\"\n",
      "\"326\",\"12/1/2020 2:00:00 AM\"\n",
      "\"321.7\",\"12/1/2020 3:00:00 AM\"\n",
      "\"333.5\",\"12/1/2020 4:00:00 AM\"\n",
      "\"373.5\",\"12/1/2020 5:00:00 AM\"\n",
      "\"451.7\",\"12/1/2020 6:00:00 AM\"\n",
      "\"493.2\",\"12/1/2020 7:00:00 AM\"\n",
      "\"494.1\",\"12/1/2020 8:00:00 AM\"\n",
      "\"476.2\",\"12/1/2020 9:00:00 AM\"\n",
      "\"482.4\",\"12/1/2020 10:00:00 AM\"\n",
      "\"468.9\",\"12/1/2020 11:00:00 AM\"\n",
      "\"466.8\",\"12/1/2020 12:00:00 PM\"\n",
      "\"508.3\",\"12/1/2020 1:00:00 PM\"\n",
      "\"524.4\",\"12/1/2020 2:0\n",
      "Fetching data for NEI00000500004004 (Hierarchy ID: 4)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000500004004. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"2163.2\",\"12/1/2020 12:00:00 AM\"\n",
      "\"2070\",\"12/1/2020 1:00:00 AM\"\n",
      "\"2043.4\",\"12/1/2020 2:00:00 AM\"\n",
      "\"2048\",\"12/1/2020 3:00:00 AM\"\n",
      "\"2100\",\"12/1/2020 4:00:00 AM\"\n",
      "\"2308.3\",\"12/1/2020 5:00:00 AM\"\n",
      "\"2684.1\",\"12/1/2020 6:00:00 AM\"\n",
      "\"2858.2\",\"12/1/2020 7:00:00 AM\"\n",
      "\"2976.2\",\"12/1/2020 8:00:00 AM\"\n",
      "\"2983.6\",\"12/1/2020 9:00:00 AM\"\n",
      "\"2991.6\",\"12/1/2020 10:00:00 AM\"\n",
      "\"2968.7\",\"12/1/2020 11:00:00 AM\"\n",
      "\"2921.2\",\"12/1/2020 12:00:00 PM\"\n",
      "\"2942.4\",\"12/1/2020 1:00:00 PM\"\n",
      "\"2958.4\",\"12\n",
      "Fetching data for NEI00000500004005 (Hierarchy ID: 5)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000500004005. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"634.7\",\"12/1/2020 12:00:00 AM\"\n",
      "\"609.6\",\"12/1/2020 1:00:00 AM\"\n",
      "\"590.3\",\"12/1/2020 2:00:00 AM\"\n",
      "\"603.7\",\"12/1/2020 3:00:00 AM\"\n",
      "\"630.8\",\"12/1/2020 4:00:00 AM\"\n",
      "\"669.6\",\"12/1/2020 5:00:00 AM\"\n",
      "\"752.2\",\"12/1/2020 6:00:00 AM\"\n",
      "\"817.2\",\"12/1/2020 7:00:00 AM\"\n",
      "\"855.1\",\"12/1/2020 8:00:00 AM\"\n",
      "\"887.4\",\"12/1/2020 9:00:00 AM\"\n",
      "\"874.2\",\"12/1/2020 10:00:00 AM\"\n",
      "\"877.4\",\"12/1/2020 11:00:00 AM\"\n",
      "\"866.6\",\"12/1/2020 12:00:00 PM\"\n",
      "\"874.3\",\"12/1/2020 1:00:00 PM\"\n",
      "\"879.5\",\"12/1/2020 2\n",
      "Fetching data for NEI00000500004006 (Hierarchy ID: 6)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000500004006. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"1035.7\",\"12/1/2020 12:00:00 AM\"\n",
      "\"1003.2\",\"12/1/2020 1:00:00 AM\"\n",
      "\"977.4\",\"12/1/2020 2:00:00 AM\"\n",
      "\"976.5\",\"12/1/2020 3:00:00 AM\"\n",
      "\"1024.4\",\"12/1/2020 4:00:00 AM\"\n",
      "\"1115.9\",\"12/1/2020 5:00:00 AM\"\n",
      "\"1300.8\",\"12/1/2020 6:00:00 AM\"\n",
      "\"1382.9\",\"12/1/2020 7:00:00 AM\"\n",
      "\"1507.1\",\"12/1/2020 8:00:00 AM\"\n",
      "\"1495.6\",\"12/1/2020 9:00:00 AM\"\n",
      "\"1479\",\"12/1/2020 10:00:00 AM\"\n",
      "\"1480.8\",\"12/1/2020 11:00:00 AM\"\n",
      "\"1492.8\",\"12/1/2020 12:00:00 PM\"\n",
      "\"1521.4\",\"12/1/2020 1:00:00 PM\"\n",
      "\"1534.8\",\"\n",
      "Fetching data for NEI00000500004007 (Hierarchy ID: 7)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000500004007. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"1380.5\",\"12/1/2020 12:00:00 AM\"\n",
      "\"1333.1\",\"12/1/2020 1:00:00 AM\"\n",
      "\"1316.4\",\"12/1/2020 2:00:00 AM\"\n",
      "\"1321.5\",\"12/1/2020 3:00:00 AM\"\n",
      "\"1370.2\",\"12/1/2020 4:00:00 AM\"\n",
      "\"1467.3\",\"12/1/2020 5:00:00 AM\"\n",
      "\"1609.2\",\"12/1/2020 6:00:00 AM\"\n",
      "\"1759.7\",\"12/1/2020 7:00:00 AM\"\n",
      "\"1797.9\",\"12/1/2020 8:00:00 AM\"\n",
      "\"1807.7\",\"12/1/2020 9:00:00 AM\"\n",
      "\"1785.1\",\"12/1/2020 10:00:00 AM\"\n",
      "\"1816.9\",\"12/1/2020 11:00:00 AM\"\n",
      "\"1814.6\",\"12/1/2020 12:00:00 PM\"\n",
      "\"1824.2\",\"12/1/2020 1:00:00 PM\"\n",
      "\"1832.\n",
      "Fetching data for NEI00000500004008 (Hierarchy ID: 8)...\n",
      "ðŸ”¹ Requesting API from 2020-12-01 to 2024-06-30\n",
      "API response received for NEI00000500004008. First 500 characters:\n",
      "\"close\",\"tradedatetimeutc\"\n",
      "\"2064.7\",\"12/1/2020 12:00:00 AM\"\n",
      "\"1990.5\",\"12/1/2020 1:00:00 AM\"\n",
      "\"1940.8\",\"12/1/2020 2:00:00 AM\"\n",
      "\"1925\",\"12/1/2020 3:00:00 AM\"\n",
      "\"1963\",\"12/1/2020 4:00:00 AM\"\n",
      "\"2122.6\",\"12/1/2020 5:00:00 AM\"\n",
      "\"2399.2\",\"12/1/2020 6:00:00 AM\"\n",
      "\"2577.2\",\"12/1/2020 7:00:00 AM\"\n",
      "\"2694.2\",\"12/1/2020 8:00:00 AM\"\n",
      "\"2710\",\"12/1/2020 9:00:00 AM\"\n",
      "\"2688.1\",\"12/1/2020 10:00:00 AM\"\n",
      "\"2733.6\",\"12/1/2020 11:00:00 AM\"\n",
      "\"2744.1\",\"12/1/2020 12:00:00 PM\"\n",
      "\"2728.8\",\"12/1/2020 1:00:00 PM\"\n",
      "\"2715.5\",\"12\n",
      "Data inserted successfully into rt_load_isone!\n",
      "Data inserted successfully into da_load_isone!\n",
      " Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "DB_PARAMS = {\n",
    "    \"dbname\": \"trueprice\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Basic VFJVRUFQSTphM3dTYVVSVA=='  \n",
    "}\n",
    "\n",
    "# Fetch Data for RT and DA Load\n",
    "RT_LOAD_MAPPING = {\n",
    "    \"NEI00009700004001\": 1,  # MAINE\n",
    "    \"NEI00009700004002\": 2,  # NEW HAMPSHIRE  \n",
    "    \"NEI00009700004003\": 3,  # VERMONT\n",
    "    \"NEI00009700004004\": 4,  # CONNECTICUT\n",
    "    \"NEI00009700004005\": 5,  # RHODE ISLAND\n",
    "    \"NEI00009700004006\": 6,  # SEMASS\n",
    "    \"NEI00009700004007\": 7,  # WC MASS\n",
    "    \"NEI00009700004008\": 8   # NEMASS BOST\n",
    "}\n",
    "\n",
    "DA_LOAD_MAPPING = {\n",
    "    \"NEI00000500004001\": 1,  # MAINE\n",
    "    \"NEI00000500004002\": 2,  # NEW HAMPSHIRE\n",
    "    \"NEI00000500004003\": 3,  # VERMONT\n",
    "    \"NEI00000500004004\": 4,  # CONNECTICUT\n",
    "    \"NEI00000500004005\": 5,  # RHODE ISLAND\n",
    "    \"NEI00000500004006\": 6,  # SEMASS\n",
    "    \"NEI00000500004007\": 7,  # WC MASS\n",
    "    \"NEI00000500004008\": 8   # NEMASS BOST\n",
    "}\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "def get_last_date(table_name):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_PARAMS)\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"SELECT MAX(date) FROM public.{table_name};\"\n",
    "        cursor.execute(query)\n",
    "        last_date = cursor.fetchone()[0]  # Fetch max date\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        if last_date is None:\n",
    "            print(f\"Database is empty. Using default start date: 2020-12-01\")\n",
    "            return datetime(2020, 12, 1)  \n",
    "        else:\n",
    "            next_date = last_date + timedelta(days=1)\n",
    "            print(f\"Found last date: {last_date}. Start from {next_date.strftime('%Y-%m-%d')}\")\n",
    "            return next_date  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching last date: {e}. Using default 2020-01-01\")\n",
    "        return datetime(2020, 12, 1)  \n",
    "\n",
    "start_date = get_last_date(\"rt_load_isone\")  \n",
    "#start_date = datetime(2020,01,01)\n",
    "#end_date = datetime.today() - timedelta(days=1)  # Ensure end_date is yesterday\n",
    "end_date = datetime(2024,6,30) \n",
    "\n",
    "print(f\"Fetching data from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "\n",
    "def fetch_data(symbol_mapping):\n",
    "    all_data = []\n",
    "    \n",
    "    for symbol, hierarchy_id in symbol_mapping.items():\n",
    "        print(f\"Fetching data for {symbol} (Hierarchy ID: {hierarchy_id})...\")\n",
    "\n",
    "        url = (f\"https://webservice.gvsi.com/api/v3/getintraday?\"\n",
    "               f\"symbols=%23{symbol}&fields=close%2Ctradedatetimeutc&output=csv&includeheaders=true\"\n",
    "               f\"&startdate={start_date.strftime('%m/%d/%Y')}&enddate={end_date.strftime('%m/%d/%Y')}\"\n",
    "               f\"&aggregatetype=0&intradaybarinterval=60&timezone=publisher\")\n",
    "\n",
    "        print(f\"ðŸ”¹ Requesting API from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"API response received for {symbol}. First 500 characters:\\n{response.text[:500]}\")\n",
    "            df = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "            df['datetime'] = pd.to_datetime(df['tradedatetimeutc'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "            df['date'] = df['datetime'].dt.strftime('%Y-%m-%d')  # Convert to YYYY-MM-DD\n",
    "            df['he'] = df['datetime'].dt.hour + 1  # Convert to HE (Hour Ending)\n",
    "            df['data'] = df['close']\n",
    "            df['hierarchy_id'] = hierarchy_id  # Assign hierarchy ID\n",
    "            df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "            df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "            df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "            df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "            df['hour_type'] = df['he'].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "            \n",
    "            def calculate_block_type(row):\n",
    "                if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "                    return \"5x16\"\n",
    "                elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "                    return \"2x16\"\n",
    "                else:\n",
    "                    return \"7x8\"\n",
    "            df['block_type'] = df.apply(calculate_block_type, axis=1)\n",
    "            df = df[['hierarchy_id', 'date', 'year', 'month', 'day', 'day_type', 'he', 'hour_type', 'block_type', 'data']]\n",
    "            all_data.append(df)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {symbol}. HTTP Status: {response.status_code}\")\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "\n",
    "RT_LOAD = fetch_data(RT_LOAD_MAPPING)\n",
    "DA_LOAD = fetch_data(DA_LOAD_MAPPING)\n",
    "\n",
    "# Push Data into PostgreSQL\n",
    "def insert_data_to_db(df, table_name):\n",
    "    if df.empty:\n",
    "        print(f\" No new data to insert for {table_name}.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Drop duplicates on the conflict key to avoid PostgreSQL ON CONFLICT error\n",
    "        df = df.drop_duplicates(subset=['hierarchy_id', 'date', 'he'], keep='last')\n",
    "\n",
    "        # Convert to native Python objects\n",
    "        df = df.astype(object)\n",
    "\n",
    "        conn = psycopg2.connect(**DB_PARAMS)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        columns = ['hierarchy_id', 'date', 'year', 'month', 'day',\n",
    "                   'day_type', 'he', 'hour_type', 'block_type', 'data']\n",
    "\n",
    "        values = df[columns].values.tolist()\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} ({', '.join(columns)})\n",
    "        VALUES %s\n",
    "        ON CONFLICT (hierarchy_id, date, he)\n",
    "        DO UPDATE SET data = EXCLUDED.data\n",
    "        \"\"\"\n",
    "\n",
    "        execute_values(cursor, insert_query, values)\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"Data inserted successfully into {table_name}!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into {table_name}: {e}\")\n",
    "# Insert Data into Database Tables\n",
    "insert_data_to_db(RT_LOAD, \"rt_load_isone\")\n",
    "insert_data_to_db(DA_LOAD, \"da_load_isone\")\n",
    "\n",
    "print(\" Process completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price Variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into `isone_vlr_price_variance` successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/trueprice\")\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch RT_LMPS and DA_LMPS Data\n",
    "query = \"\"\"\n",
    "    SELECT rt.hierarchy_id, rt.date, rt.he, rt.data AS rt_data, da.data AS da_data\n",
    "    FROM rt_lmps_isone rt\n",
    "    JOIN da_lmps_isone da \n",
    "    ON rt.hierarchy_id = da.hierarchy_id \n",
    "    AND rt.date = da.date \n",
    "    AND rt.he = da.he\n",
    "    WHERE rt.hierarchy_id BETWEEN 1 AND 8\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine) \n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "df[\"hour_type\"] = df[\"he\"].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "\n",
    "\n",
    "def calculate_block_type(row):\n",
    "    if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"5x16\"\n",
    "    elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"2x16\"\n",
    "    else:\n",
    "        return \"7x8\"\n",
    "\n",
    "df[\"block_type\"] = df.apply(calculate_block_type, axis=1)\n",
    "\n",
    "\n",
    "df[\"data\"] = df.apply(lambda row: (row[\"rt_data\"] - row[\"da_data\"]) if row[\"hierarchy_id\"] <= 4 \n",
    "                      else (row[\"rt_data\"] - row[\"da_data\"]) / (row[\"da_data\"] if row[\"da_data\"] != 0 else 1), axis=1).round(6)\n",
    "\n",
    "\n",
    "df = df[[\"hierarchy_id\", \"date\", \"year\", \"month\", \"day\", \"day_type\", \"he\", \"hour_type\", \"block_type\", \"data\"]]\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"trueprice\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "    INSERT INTO price_variance_isone \n",
    "    (hierarchy_id, date, year, month, day, day_type, he, hour_type, block_type, data)\n",
    "    VALUES (%(hierarchy_id)s, %(date)s, %(year)s, %(month)s, %(day)s, %(day_type)s, %(he)s, %(hour_type)s, %(block_type)s, %(data)s)\n",
    "    ON CONFLICT (hierarchy_id, date, he) \n",
    "    DO UPDATE SET data = EXCLUDED.data\n",
    "\"\"\", df.to_dict(orient=\"records\"))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data inserted into `isone_vlr_price_variance` successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into `load_variance_isone` successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/trueprice\")\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT rt.hierarchy_id, rt.date, rt.he, rt.data AS rt_data, da.data AS da_data\n",
    "    FROM rt_load_isone rt\n",
    "    JOIN da_load_isone da \n",
    "    ON rt.hierarchy_id = da.hierarchy_id \n",
    "    AND rt.date = da.date \n",
    "    AND rt.he = da.he\n",
    "    WHERE rt.hierarchy_id BETWEEN 1 AND 8\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine) \n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "df[\"hour_type\"] = df[\"he\"].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "\n",
    "def calculate_block_type(row):\n",
    "    if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"5x16\"\n",
    "    elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"2x16\"\n",
    "    else:\n",
    "        return \"7x8\"\n",
    "\n",
    "df[\"block_type\"] = df.apply(calculate_block_type, axis=1)\n",
    "\n",
    "df[\"data\"] = df.apply(lambda row: (row[\"rt_data\"] - row[\"da_data\"]) if row[\"hierarchy_id\"] <= 4 \n",
    "                      else (row[\"rt_data\"] - row[\"da_data\"]) / (row[\"da_data\"] if row[\"da_data\"] != 0 else 1), axis=1).round(6)\n",
    "\n",
    "df = df[[\"hierarchy_id\", \"date\", \"year\", \"month\", \"day\", \"day_type\", \"he\", \"hour_type\", \"block_type\", \"data\"]]\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"trueprice\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "    INSERT INTO load_variance_isone\n",
    "    (hierarchy_id, date, year, month, day, day_type, he, hour_type, block_type, data)\n",
    "    VALUES (%(hierarchy_id)s, %(date)s, %(year)s, %(month)s, %(day)s, %(day_type)s, %(he)s, %(hour_type)s, %(block_type)s, %(data)s)\n",
    "    ON CONFLICT (hierarchy_id, date, he) \n",
    "    DO UPDATE SET data = EXCLUDED.data\n",
    "\"\"\", df.to_dict(orient=\"records\"))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data inserted into `load_variance_isone` successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Hedge Cost ( without Imbalance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into `ercot_vlr_tot_hedgecostwoimbalance` successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/trueprice\")\n",
    "\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT da_lmps.hierarchy_id, da_lmps.date, da_lmps.he, \n",
    "           da_lmps.data AS da_lmps, da_load.data AS da_load\n",
    "    FROM da_lmps_isone da_lmps\n",
    "    JOIN da_load_isone da_load \n",
    "    ON da_lmps.hierarchy_id = da_load.hierarchy_id \n",
    "    AND da_lmps.date = da_load.date \n",
    "    AND da_lmps.he = da_load.he\n",
    "    WHERE da_lmps.hierarchy_id BETWEEN 1 AND 8\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)  \n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "df[\"hour_type\"] = df[\"he\"].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "\n",
    "def calculate_block_type(row):\n",
    "    if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"5x16\"\n",
    "    elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"2x16\"\n",
    "    else:\n",
    "        return \"7x8\"\n",
    "\n",
    "df[\"block_type\"] = df.apply(calculate_block_type, axis=1)\n",
    "\n",
    "df[\"data\"] = (df[\"da_lmps\"] * df[\"da_load\"]).round(6)\n",
    "df = df[[\"hierarchy_id\", \"date\", \"year\", \"month\", \"day\", \"day_type\", \"he\", \"hour_type\", \"block_type\", \"data\"]]\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"trueprice\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "    INSERT INTO isone_vlr_tot_hedgecostwoimbalance \n",
    "    (hierarchy_id, date, year, month, day, day_type, he, hour_type, block_type, data)\n",
    "    VALUES (%(hierarchy_id)s, %(date)s, %(year)s, %(month)s, %(day)s, %(day_type)s, %(he)s, %(hour_type)s, %(block_type)s, %(data)s)\n",
    "    ON CONFLICT (hierarchy_id, date, he) \n",
    "    DO UPDATE SET data = EXCLUDED.data\n",
    "\"\"\", df.to_dict(orient=\"records\"))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data inserted into `ercot_vlr_tot_hedgecostwoimbalance` successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total cost of imbalance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into `isone_tot_costofimbalance` successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/trueprice\")\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "#  Fetch Load Variance and RT_LMPS Data\n",
    "query = \"\"\"\n",
    "    SELECT lv.hierarchy_id, lv.date, lv.he, \n",
    "           lv.data AS load_variance, rt_lmps.data AS rt_lmps\n",
    "    FROM load_variance_isone lv\n",
    "    JOIN rt_lmps_isone rt_lmps \n",
    "    ON lv.hierarchy_id = rt_lmps.hierarchy_id \n",
    "    AND lv.date = rt_lmps.date \n",
    "    AND lv.he = rt_lmps.he\n",
    "    WHERE lv.hierarchy_id BETWEEN 1 AND 8\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine) \n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "df[\"hour_type\"] = df[\"he\"].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "\n",
    "def calculate_block_type(row):\n",
    "    if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"5x16\"\n",
    "    elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"2x16\"\n",
    "    else:\n",
    "        return \"7x8\"\n",
    "\n",
    "df[\"block_type\"] = df.apply(calculate_block_type, axis=1)\n",
    "\n",
    "df[\"data\"] = (df[\"load_variance\"] * df[\"rt_lmps\"]).round(6)\n",
    "\n",
    "df = df[[\"hierarchy_id\", \"date\", \"year\", \"month\", \"day\", \"day_type\", \"he\", \"hour_type\", \"block_type\", \"data\"]]\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"trueprice\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "    INSERT INTO isone_tot_costofimbalance \n",
    "    (hierarchy_id, date, year, month, day, day_type, he, hour_type, block_type, data)\n",
    "    VALUES (%(hierarchy_id)s, %(date)s, %(year)s, %(month)s, %(day)s, %(day_type)s, %(he)s, %(hour_type)s, %(block_type)s, %(data)s)\n",
    "    ON CONFLICT (hierarchy_id, date, he) \n",
    "    DO UPDATE SET data = EXCLUDED.data\n",
    "\"\"\", df.to_dict(orient=\"records\"))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data inserted into `isone_tot_costofimbalance` successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Hedge Cost With Imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into `isone_vlr_tot_hedgecostwimbalance` successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/trueprice\")\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT hcwi.hierarchy_id, hcwi.date, hcwi.he, \n",
    "           hcwi.data AS hedgecostwoimbalance, ci.data AS costofimbalance\n",
    "    FROM isone_vlr_tot_hedgecostwoimbalance hcwi\n",
    "    JOIN isone_tot_costofimbalance ci\n",
    "    ON hcwi.hierarchy_id = ci.hierarchy_id \n",
    "    AND hcwi.date = ci.date \n",
    "    AND hcwi.he = ci.he\n",
    "    WHERE hcwi.hierarchy_id BETWEEN 1 AND 8\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)  \n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "df[\"hour_type\"] = df[\"he\"].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "\n",
    "def calculate_block_type(row):\n",
    "    if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"5x16\"\n",
    "    elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"2x16\"\n",
    "    else:\n",
    "        return \"7x8\"\n",
    "\n",
    "df[\"block_type\"] = df.apply(calculate_block_type, axis=1)\n",
    "\n",
    "df[\"data\"] = (df[\"hedgecostwoimbalance\"] + df[\"costofimbalance\"]).round(6)\n",
    "\n",
    "df = df[[\"hierarchy_id\", \"date\", \"year\", \"month\", \"day\", \"day_type\", \"he\", \"hour_type\", \"block_type\", \"data\"]]\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"trueprice\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "    INSERT INTO isone_vlr_tot_hedgecostwimbalance \n",
    "    (hierarchy_id, date, year, month, day, day_type, he, hour_type, block_type, data)\n",
    "    VALUES (%(hierarchy_id)s, %(date)s, %(year)s, %(month)s, %(day)s, %(day_type)s, %(he)s, %(hour_type)s, %(block_type)s, %(data)s)\n",
    "    ON CONFLICT (hierarchy_id, date, he) \n",
    "    DO UPDATE SET data = EXCLUDED.data\n",
    "\"\"\", df.to_dict(orient=\"records\"))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data inserted into `isone_vlr_tot_hedgecostwimbalance` successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Price W Imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data inserted into `isone_vlr_tot_pricewimbalance` successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/trueprice\")\n",
    "\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "# Fetch Hedge Cost with Imbalance and RT Load Data\n",
    "query = \"\"\"\n",
    "    SELECT hcw.hierarchy_id, hcw.date, hcw.he, \n",
    "           hcw.data AS hedgecostwimbalance, rt_load.data AS rt_load\n",
    "    FROM isone_vlr_tot_hedgecostwimbalance hcw\n",
    "    JOIN rt_load_isone rt_load\n",
    "    ON hcw.hierarchy_id = rt_load.hierarchy_id \n",
    "    AND hcw.date = rt_load.date \n",
    "    AND hcw.he = rt_load.he\n",
    "    WHERE hcw.hierarchy_id BETWEEN 1 AND 8\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)  \n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "df[\"hour_type\"] = df[\"he\"].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "\n",
    "def calculate_block_type(row):\n",
    "    if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"5x16\"\n",
    "    elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"2x16\"\n",
    "    else:\n",
    "        return \"7x8\"\n",
    "\n",
    "df[\"block_type\"] = df.apply(calculate_block_type, axis=1)\n",
    "\n",
    "df[\"data\"] = df.apply(lambda row: (row[\"hedgecostwimbalance\"] / row[\"rt_load\"]) \n",
    "                      if row[\"rt_load\"] != 0 else 0, axis=1)  # Handles division by zero\n",
    "\n",
    "df[\"data\"] = df[\"data\"].round(6)\n",
    "\n",
    "\n",
    "df = df[[\"hierarchy_id\", \"date\", \"year\", \"month\", \"day\", \"day_type\", \"he\", \"hour_type\", \"block_type\", \"data\"]]\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"trueprice\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "    INSERT INTO isone_vlr_tot_pricewimbalance \n",
    "    (hierarchy_id, date, year, month, day, day_type, he, hour_type, block_type, data)\n",
    "    VALUES (%(hierarchy_id)s, %(date)s, %(year)s, %(month)s, %(day)s, %(day_type)s, %(he)s, %(hour_type)s, %(block_type)s, %(data)s)\n",
    "    ON CONFLICT (hierarchy_id, date, he) \n",
    "    DO UPDATE SET data = EXCLUDED.data\n",
    "\"\"\", df.to_dict(orient=\"records\"))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\" Data inserted into `isone_vlr_tot_pricewimbalance` successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilized VLR Cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into `isone_vlr_unitized_cost` successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/trueprice\")\n",
    "\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch Total Price with Imbalance and DA LMPS Data\n",
    "query = \"\"\"\n",
    "    SELECT tpw.hierarchy_id, tpw.date, tpw.he, \n",
    "           tpw.data AS total_price_wimbalance, da_lmps.data AS da_lmps\n",
    "    FROM isone_vlr_tot_pricewimbalance tpw\n",
    "    JOIN da_lmps_isone da_lmps\n",
    "    ON tpw.hierarchy_id = da_lmps.hierarchy_id \n",
    "    AND tpw.date = da_lmps.date \n",
    "    AND tpw.he = da_lmps.he\n",
    "    WHERE tpw.hierarchy_id BETWEEN 1 AND 8\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)  \n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "df[\"hour_type\"] = df[\"he\"].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "\n",
    "def calculate_block_type(row):\n",
    "    if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"5x16\"\n",
    "    elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"2x16\"\n",
    "    else:\n",
    "        return \"7x8\"\n",
    "\n",
    "df[\"block_type\"] = df.apply(calculate_block_type, axis=1)\n",
    "\n",
    "df[\"data\"] = df.apply(lambda row: (row[\"total_price_wimbalance\"] - row[\"da_lmps\"])\n",
    "                      if row[\"da_lmps\"] is not None else 0, axis=1)  \n",
    "\n",
    "df[\"data\"] = df[\"data\"].round(6)  \n",
    "\n",
    "df = df[[\"hierarchy_id\", \"date\", \"year\", \"month\", \"day\", \"day_type\", \"he\", \"hour_type\", \"block_type\", \"data\"]]\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"trueprice\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "    INSERT INTO isone_vlr_unitized_cost \n",
    "    (hierarchy_id, date, year, month, day, day_type, he, hour_type, block_type, data)\n",
    "    VALUES (%(hierarchy_id)s, %(date)s, %(year)s, %(month)s, %(day)s, %(day_type)s, %(he)s, %(hour_type)s, %(block_type)s, %(data)s)\n",
    "    ON CONFLICT (hierarchy_id, date, he) \n",
    "    DO UPDATE SET data = EXCLUDED.data\n",
    "\"\"\", df.to_dict(orient=\"records\"))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data inserted into `isone_vlr_unitized_cost` successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hourly VLR Cost (%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into `isone_vlr_hourly_cost` successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/trueprice\")\n",
    "\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "\n",
    "#  Fetch Utilized Cost and DA LMPS Data\n",
    "query = \"\"\"\n",
    "    SELECT uc.hierarchy_id, uc.date, uc.he, \n",
    "           uc.data AS utilized_cost, da_lmps.data AS da_lmps\n",
    "    FROM isone_vlr_unitized_cost uc\n",
    "    JOIN da_lmps_isone da_lmps\n",
    "    ON uc.hierarchy_id = da_lmps.hierarchy_id \n",
    "    AND uc.date = da_lmps.date \n",
    "    AND uc.he = da_lmps.he\n",
    "    WHERE uc.hierarchy_id BETWEEN 1 AND 8\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)  \n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "df[\"hour_type\"] = df[\"he\"].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "\n",
    "def calculate_block_type(row):\n",
    "    if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"5x16\"\n",
    "    elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"2x16\"\n",
    "    else:\n",
    "        return \"7x8\"\n",
    "\n",
    "df[\"block_type\"] = df.apply(calculate_block_type, axis=1)\n",
    "\n",
    "df[\"data\"] = df.apply(lambda row: (row[\"utilized_cost\"] / row[\"da_lmps\"]) \n",
    "                      if row[\"da_lmps\"] != 0 else 0, axis=1)  \n",
    "\n",
    "df[\"data\"] = df[\"data\"].round(6)\n",
    "\n",
    "\n",
    "df = df[[\"hierarchy_id\", \"date\", \"year\", \"month\", \"day\", \"day_type\", \"he\", \"hour_type\", \"block_type\", \"data\"]]\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"trueprice\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "    INSERT INTO isone_vlr_hourly_cost \n",
    "    (hierarchy_id, date, year, month, day, day_type, he, hour_type, block_type, data)\n",
    "    VALUES (%(hierarchy_id)s, %(date)s, %(year)s, %(month)s, %(day)s, %(day_type)s, %(he)s, %(hour_type)s, %(block_type)s, %(data)s)\n",
    "    ON CONFLICT (hierarchy_id, date, he) \n",
    "    DO UPDATE SET data = EXCLUDED.data\n",
    "\"\"\", df.to_dict(orient=\"records\"))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data inserted into `isone_vlr_hourly_cost` successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VLR COST (%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into `isone_vlr` successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/trueprice\")\n",
    "\n",
    "DAY_TYPE_MAPPING = {\n",
    "    1: \"WeekEnd\",  # Sunday\n",
    "    2: \"WeekDay\",  # Monday\n",
    "    3: \"WeekDay\",  # Tuesday\n",
    "    4: \"WeekDay\",  # Wednesday\n",
    "    5: \"WeekDay\",  # Thursday\n",
    "    6: \"WeekDay\",  # Friday\n",
    "    7: \"WeekEnd\"   # Saturday\n",
    "}\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT hierarchy_id, date, he, data AS hourly_vlr_cost\n",
    "    FROM isone_vlr_hourly_cost\n",
    "    WHERE hierarchy_id BETWEEN 1 AND 8\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)  \n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df['day'] = pd.to_datetime(df['date']).apply(lambda x: x.isoweekday() % 7 + 1)\n",
    "df['day_type'] = df['day'].map(DAY_TYPE_MAPPING)\n",
    "df[\"hour_type\"] = df[\"he\"].apply(lambda x: \"OnPeak\" if 7 <= x <= 22 else \"OffPeak\")\n",
    "\n",
    "def calculate_block_type(row):\n",
    "    if row[\"day_type\"] == \"WeekDay\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"5x16\"\n",
    "    elif row[\"day_type\"] == \"WeekEnd\" and row[\"hour_type\"] == \"OnPeak\":\n",
    "        return \"2x16\"\n",
    "    else:\n",
    "        return \"7x8\"\n",
    "\n",
    "df[\"block_type\"] = df.apply(calculate_block_type, axis=1)\n",
    "\n",
    "df[\"concat_string\"] = df[\"month\"].astype(str) + \"_\" + df[\"he\"].astype(str) + \"_\" + df[\"block_type\"]\n",
    "\n",
    "def mode_function(x):\n",
    "    return x.mode()[0] if not x.mode().empty else None\n",
    "\n",
    "agg_df = df.groupby([\"hierarchy_id\", \"concat_string\"]).agg(\n",
    "    {\n",
    "        \"date\": \"first\",  \n",
    "        \"year\": \"first\",\n",
    "        \"month\": \"first\",\n",
    "        \"day\": \"first\", \n",
    "        \"day_type\": mode_function,\n",
    "        \"hourly_vlr_cost\": \"mean\",  # Mean for VLR cost\n",
    "        \"he\": \"mean\",  # Mean for Hour\n",
    "        \"hour_type\": mode_function,  # Mode for HourType\n",
    "        \"block_type\": mode_function  # Mode for BlockType\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "agg_df = agg_df.sort_values(by=['hierarchy_id', 'month', 'he'])\n",
    "\n",
    "agg_df.rename(columns={\"hourly_vlr_cost\": \"data\"}, inplace=True)\n",
    "\n",
    "agg_df[\"data\"] = agg_df[\"data\"].round(6)  \n",
    "\n",
    "agg_df = agg_df[[\"hierarchy_id\", \"date\", \"year\", \"month\", \"day\", \"day_type\", \"he\", \"hour_type\", \"block_type\", \"data\"]]\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"trueprice\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "    INSERT INTO isone_vlr \n",
    "    (hierarchy_id, date, year, month, day, day_type, he, hour_type, block_type, data)\n",
    "    VALUES (%(hierarchy_id)s, %(date)s, %(year)s, %(month)s, %(day)s, %(day_type)s, %(he)s, %(hour_type)s, %(block_type)s, %(data)s)\n",
    "    ON CONFLICT (hierarchy_id, date, he) \n",
    "    DO UPDATE SET \n",
    "        year = EXCLUDED.year,\n",
    "        month = EXCLUDED.month,\n",
    "        day = EXCLUDED.day,\n",
    "        day_type = EXCLUDED.day_type,\n",
    "        hour_type = EXCLUDED.hour_type,\n",
    "        block_type = EXCLUDED.block_type,\n",
    "        data = EXCLUDED.data\n",
    "\"\"\", agg_df.to_dict(orient=\"records\"))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data inserted into `isone_vlr` successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
