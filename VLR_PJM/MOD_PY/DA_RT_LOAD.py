# DA_RT_LOAD.py - autogenerated file
import os
from dotenv import load_dotenv
import requests
import pandas as pd
from io import StringIO
from datetime import datetime, timedelta
import psycopg2
import psycopg2.extras

# Load environment variables
load_dotenv(override=True)

DB_PARAMS = {
    "dbname": os.getenv("DB_NAME"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "host": os.getenv("DB_HOST"),
    "port": os.getenv("DB_PORT")
}

headers = {
    'Authorization': 'Basic VFJVRUFQSTphM3dTYVVSVA=='  
}

# SQLAlchemy engine for reading data

def get_last_date(table_name):
    try:
        conn = psycopg2.connect(**DB_PARAMS)
        cursor = conn.cursor()
        query = f"SELECT MAX(date) FROM public.{table_name};"
        cursor.execute(query)
        last_date = cursor.fetchone()[0]

        cursor.close()
        conn.close()

        if last_date is None:
            print("Database is empty. Using default start date: 2020-12-01")
            return datetime(2020, 12, 1)
        else:
            next_date = last_date + timedelta(days=1)
            print(f"Found last date: {last_date}. Start from {next_date.strftime('%Y-%m-%d')}")
            return next_date  

    except Exception as e:
        print(f"Error fetching last date: {e}. Using default 2020-12-01")
        return datetime(2020, 12, 1)  
start_date = get_last_date("pjm_vlr_rt_load")
# end_date = datetime(2025,4,30)
end_date = datetime(2025,5,15)

def fetch_data(symbol_mapping):
    """Fetches data from the API for the given symbols and hierarchy IDs, summing values for duplicate keys."""
    all_data = []
    
    for symbols, hierarchy_id in symbol_mapping.items():
        if isinstance(symbols, (list, tuple)):  # Updated check to include tuples
            combined_data = []  # List to store multiple results

            for symbol in symbols:
                print(f"Fetching data for {symbol} (Hierarchy ID: {hierarchy_id})...")

                url = (f"https://webservice.gvsi.com/api/v3/getintraday?"
                       f"symbols=%23{symbol}&fields=close%2Ctradedatetimeutc&output=csv&includeheaders=true"
                       f"&startdate={start_date.strftime('%m/%d/%Y')}&enddate={end_date.strftime('%m/%d/%Y')}"
                       f"&aggregatetype=0&intradaybarinterval=60&timezone=publisher")

                print(f"üîπ Requesting API from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}")

                response = requests.get(url, headers=headers, timeout=90)

                if response.status_code == 200:
                    response_text = response.text.strip()
                    print(f"API response received for {symbol} (First 500 chars):\n{response_text[:500]}")

                    if not response_text or "Error" in response_text:
                        print(f"‚ö†Ô∏è API returned an error or empty response for {symbol}. Skipping...")
                        continue

                    try:
                        df = pd.read_csv(StringIO(response_text), skipinitialspace=True)
                        expected_columns = {'tradedatetimeutc', 'close'}
                        if not expected_columns.issubset(df.columns):
                            print(f"‚ö†Ô∏è Unexpected data format for {symbol}. Skipping...")
                            continue

                        df['datetime'] = pd.to_datetime(df['tradedatetimeutc'], errors='coerce')
                        df = df.dropna(subset=['datetime'])
                        df['date'] = df['datetime'].dt.strftime('%Y-%m-%d')
                        df['he'] = df['datetime'].dt.hour + 1
                        df['data'] = df['close']
                        df['hierarchy_id'] = hierarchy_id

                        df = df[['hierarchy_id', 'date', 'he', 'data']]
                        combined_data.append(df)
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error processing data for {symbol}: {e}")
            
            if combined_data:
                combined_df = pd.concat(combined_data).groupby(['hierarchy_id', 'date', 'he'], as_index=False).sum()
                all_data.append(combined_df)
        else:
            # Process a single symbol
            symbol = symbols
            # print(f"Fetching data for {symbol} (Hierarchy ID: {hierarchy_id})...")

            url = (f"https://webservice.gvsi.com/api/v3/getintraday?"
                   f"symbols=%23{symbol}&fields=close%2Ctradedatetimeutc&output=csv&includeheaders=true"
                   f"&startdate={start_date.strftime('%m/%d/%Y')}&enddate={end_date.strftime('%m/%d/%Y')}"
                   f"&aggregatetype=0&intradaybarinterval=60&timezone=publisher")

            print(f"üîπ Requesting API from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}")

            response = requests.get(url, headers=headers, timeout=90)

            if response.status_code == 200:
                df = pd.read_csv(StringIO(response.text), skipinitialspace=True)

                df['datetime'] = pd.to_datetime(df['tradedatetimeutc'], errors='coerce')
                df = df.dropna(subset=['datetime'])
                df['date'] = df['datetime'].dt.strftime('%Y-%m-%d')
                df['he'] = df['datetime'].dt.hour + 1
                df['data'] = df['close']
                df['hierarchy_id'] = hierarchy_id

                df = df[['hierarchy_id', 'date', 'he', 'data']]
                all_data.append(df)

    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()
PJM_RT_LOAD_MAPPING = {
    "PJM00000143000000051291": 1,  # AECO
    "PJM00000143000970242671": 2,  # AEP  
    "PJM00000143000970242672": 3,  # APS    
    ("PJM00000143000970242676", "PJM00000143000970242675"): 4,  # ATSI (Combined)
    "PJM00000143000000051292": 5,  # BGE
    "PJM00000143000970242673": 6,  # COMED
    "PJM00000143000034508503": 7,  # DAY
    "PJM00000143000124076095": 8,  # DEOK
    "PJM00000143000034964545": 9,  # DOM
    "PJM00000143000970242674": 10, # DPL
    "PJM00000143000037737283": 11, # DUQ
    "PJM00000143000000051295": 12, # JCPL
    "PJM00000143000000051296": 13, # METED
    "PJM00000143000000051297": 14, # PECO
    "PJM00000143000000051300": 15, # PENELEC
    "PJM00000143000000051298": 16, # PEPCO
    "PJM00000143000970242677": 17, # PPL
    "PJM00000143000000051301": 18, # PSEG
    "PJM00000143000007633629": 19  # RECO
}



# Mapping for Day-Ahead Load Data
PJM_DA_LOAD_MAPPING = {
    "PJM00000025000000000087S": 1,  
    "PJM00000025000000000100S": 2,  
    "PJM00000025000008445784": 3,  
    "PJM00000025000116013753": 4,  
    "PJM00000025000000000095S": 5,  
    "PJM00000025000033092371": 6,  
    "PJM00000025000034508503": 7,  
    "PJM00000025000124076095": 8,  
    "PJM00000025000000000089S": 9,  
    "PJM00000025000000000101S": 10, 
    "PJM00000025000037737283": 11, 
    "PJM00000025000000000096S": 12, 
    "PJM00000025000000000097S": 13, 
    "PJM00000025000000000092S": 14, 
    "PJM00000025000000000098S": 15, 
    "PJM00000025000000000099S": 16, 
    "PJM00000025000000000093S": 17, 
    "PJM00000025000000000091S": 18, 
    "PJM00000025000000000102S": 19  
}

# Fetch data
RT_LOAD = fetch_data(PJM_RT_LOAD_MAPPING)
DA_LOAD = fetch_data(PJM_DA_LOAD_MAPPING)

def insert_data_to_db(df, table_name):
    if df.empty:
        print(f"‚ö† No new data to insert for {table_name}.")
        return

    try:
        conn = psycopg2.connect(**DB_PARAMS)
        cursor = conn.cursor()

        for _, row in df.iterrows():
            insert_query = f"""
                INSERT INTO {table_name} (hierarchy_id, date, he, data)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (hierarchy_id, date, he)
                DO NOTHING;
            """
            cursor.execute(insert_query, (row['hierarchy_id'], row['date'], row['he'], row['data']))

        conn.commit()
        cursor.close()
        conn.close()
        print(f" Data inserted successfully into {table_name} (duplicates skipped)!")
        
    except Exception as e:
        print(f" Error inserting data into {table_name}: {e}")


insert_data_to_db(RT_LOAD, "pjm_vlr_rt_load")
insert_data_to_db(DA_LOAD, "pjm_vlr_da_load")
